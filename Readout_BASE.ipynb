{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf4ae0a-5eca-4336-960d-5d4967d1d486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/kit/stud/updzl\n",
      "/pfs/data5/home/kit/stud/updzl\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAABECAYAAABkg28iAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAC7klEQVR4nO3av0rVcRjH8eeoeXLITIr+bC4RtRyI7qHB5hpCoRto7wLauoUCt2htcoiWIgriRI66NChClmmmxzz+ugEVeuDr1yOv1/osHzhn+L7h12qapgkAAAD+y1DtAQAAAINITAEAACSIKQAAgAQxBQAAkCCmAAAAEkaOOr7/+CmW1veOa8uxuzX+J4Z2VmrPKGZtbCpWdrdqzyji5uZqNONXo7VxOn+/ZuJGxNZ+7RnFNOf+RvQ2a88oZmTscoz2h2vPKGajtR29Xq/2jCImJ9Zirz8ZI8M/ak8pYr03FRv7p/e/2e7vxMpWv/aMYrxbBpd3y2AbvtSOTqdz4O3ImNpvImbnv5fYdCJ8nv4V57uPas8oZuH265hdnK89o4ivb5/Fzv25OPtypvaUIrYffoh487v2jHLu7kbz5VXtFcWM3nkc11cv1p5RzLsLi7GwsFB7RhEP7r2I5Z9P4tqFp7WnFPFt+XnMrV2pPaOY6TNL3i0DzLtlcJ32d0t7ZvzQm8/8AAAAEsQUAABAgpgCAABIEFMAAAAJYgoAACBBTAEAACSIKQAAgAQxBQAAkCCmAAAAEsQUAABAgpgCAABIEFMAAAAJYgoAACBBTAEAACSIKQAAgAQxBQAAkCCmAAAAEsQUAABAgpgCAABIEFMAAAAJYgoAACBBTAEAACSIKQAAgAQxBQAAkCCmAAAAEsQUAABAgpgCAABIEFMAAAAJYgoAACBBTAEAACSIKQAAgAQxBQAAkCCmAAAAEsQUAABAgpgCAABIEFMAAAAJYgoAACBBTAEAACSIKQAAgAQxBQAAkCCmAAAAEsQUAABAgpgCAABIEFMAAAAJYgoAACBBTAEAACSIKQAAgAQxBQAAkCCmAAAAEsQUAABAgpgCAABIEFMAAAAJYgoAACCh1TRNc9ix2+1Gu90+zj0AAAAnRq/Xi06nc+DtyJgCAADgYD7zAwAASBBTAAAACWIKAAAgQUwBAAAkiCkAAICEf7XPhwH1AInuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import pandas as pd\n",
    "import torch\n",
    "#from google.colab import drive\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader\n",
    "#!pip install torch pytorch-lightning\n",
    "#import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "#import torchvision\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "###\n",
    "# Ralf Loritz 15.2.2022\n",
    "# Rainfall-runoff using LSTM Models\n",
    "# Pytorch\n",
    "###\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2 #Reload all modules every time before executing the Python code typed.\n",
    "\n",
    "# built-in\n",
    "import os\n",
    "import importlib\n",
    "import math\n",
    "import collections\n",
    "###\n",
    "# addtional packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# set torch seed\n",
    "def setup_seed(seed):\n",
    "    random.seed(seed)                          \n",
    "    np.random.seed(seed)                       \n",
    "    torch.manual_seed(seed)                    \n",
    "    torch.cuda.manual_seed(seed)               \n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "\n",
    "seed=330000\n",
    "setup_seed(seed)\n",
    "\n",
    "# import from sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "\n",
    "# import own loss functions\n",
    "#from src.loss_functions import kge, kgeprime, nse, rmse\n",
    "#from src.ML_functions import onehot_encode_pd, lagged_features, get_scaler\n",
    "#from src.ML_train import train_model\n",
    "\n",
    "#-----------------------\n",
    "#import src.ML_train\n",
    "#importlib.reload(src.ML_train)\n",
    "#from src.ML_train import train_model\n",
    "#----------------------\n",
    "\n",
    "\n",
    "###\n",
    "# set plot style seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# set colors\n",
    "current_palette = sns.color_palette(\"colorblind\", 15)\n",
    "sns.palplot(current_palette)\n",
    "print(os.getcwd())\n",
    "# set workspcae\n",
    "os.chdir('./')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab251ec-fc32-495f-bea4-a9839cbf461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drive.mount('/content/drive')\n",
    "\n",
    "###\n",
    "# identify GPU\n",
    "#print(torch.cuda.device_count())\n",
    "#print(torch.cuda.get_device_name(0))\n",
    "# set cuda0 as GPU\n",
    "cuda0 = torch.device('cuda:0')\n",
    "cuda1 = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d1a144b-bd4a-40fa-8d18-ca252e0f3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing etc.\n",
    "\n",
    "\n",
    "# Creates a training, validation and test data set\n",
    "# Check https://onlinelibrary.wiley.com/doi/abs/10.1029/2021WR031523 for split sampling approaches in hydrological modeling\n",
    "# Validation period is usually very small and is used to check the performance of the model during the training on a out-of-sample batch\n",
    "# to monitor evtl. overfitting\n",
    "def split_data(start_train, end_train, start_val, end_val, start_test, end_test, df):\n",
    "  ''' split data set into training period, validation period (for hyperparameter tuning) and independent test period.\n",
    "  periods should correlate with benchmark model. \n",
    "  input in date time format or any other indexing used in df(dataframe)'''\n",
    "  train = df.loc[start_train:end_train]\n",
    "  val = df.loc[start_val:end_val]\n",
    "  test = df.loc[start_test:end_test]\n",
    "  return train, val, test\n",
    "\n",
    "# Standardizes data for the network, to avoid scaling issues. Uses own metrics.\n",
    "def local_standartization(data):\n",
    "  ''' z-standartisation on the the all the data, statistic values taken from training period'''\n",
    "  stds = data.std()\n",
    "  mean = data.mean()\n",
    "  scaled_data = (data-mean)/stds\n",
    "  return scaled_data, stds, mean\n",
    "\n",
    "# Standartizes data for network with given metrics(from training data set)\n",
    "def scale(data, stds, mean):\n",
    "  scaled_data = (data-mean)/stds\n",
    "  return scaled_data\n",
    "\n",
    "# Performance metrics:\n",
    "# NSE function\n",
    "def get_nse(y_test, predictions):\n",
    "  assert len(y_test) == len(predictions)  \n",
    "  numerator = sum([(y_test[i]-predictions[i])**2 for i in range(len(y_test))])\n",
    "  y_test_avg = sum(y_test)/len(y_test)\n",
    "  denominator = sum([(y_test[i]-y_test_avg)**2 for i in range(len(y_test))])\n",
    "  NSE = 1- (numerator/denominator)\n",
    "  return NSE\n",
    "\n",
    "# NSE1 as loss function \n",
    "def NSE1_loss(obs, target):\n",
    "    obs_mean = torch.mean(obs)\n",
    "    denominator = torch.sum(abs(torch.sub(obs, obs_mean)))\n",
    "    nominator = torch.sum(abs(torch.sub(obs, target)))\n",
    "    loss = nominator/denominator\n",
    "    return loss\n",
    "\n",
    "# KGE 2009\n",
    "def kge(simulations, evaluation):\n",
    "    \"\"\"Original Kling-Gupta Efficiency (KGE) and its three components\n",
    "    (r, α, β) as per `Gupta et al., 2009\n",
    "    <https://doi.org/10.1016/j.jhydrol.2009.08.003>`_.\n",
    "    Note, all four values KGE, r, α, β are returned, in this order.\n",
    "    :Calculation Details:\n",
    "        .. math::\n",
    "           E_{\\\\text{KGE}} = 1 - \\\\sqrt{[r - 1]^2 + [\\\\alpha - 1]^2\n",
    "           + [\\\\beta - 1]^2}\n",
    "        .. math::\n",
    "           r = \\\\frac{\\\\text{cov}(e, s)}{\\\\sigma({e}) \\\\cdot \\\\sigma(s)}\n",
    "        .. math::\n",
    "           \\\\alpha = \\\\frac{\\\\sigma(s)}{\\\\sigma(e)}\n",
    "        .. math::\n",
    "           \\\\beta = \\\\frac{\\\\mu(s)}{\\\\mu(e)}\n",
    "        where *e* is the *evaluation* series, *s* is (one of) the\n",
    "        *simulations* series, *cov* is the covariance, *σ* is the\n",
    "        standard deviation, and *μ* is the arithmetic mean.\n",
    "    \"\"\"\n",
    "    # calculate error in timing and dynamics r\n",
    "    # (Pearson's correlation coefficient)\n",
    "    sim_mean = np.mean(simulations, axis=0, dtype=np.float64)\n",
    "    obs_mean = np.mean(evaluation, dtype=np.float64)\n",
    "\n",
    "    r_num = np.sum((simulations - sim_mean) * (evaluation - obs_mean),\n",
    "                   axis=0, dtype=np.float64)\n",
    "    r_den = np.sqrt(np.sum((simulations - sim_mean) ** 2,\n",
    "                           axis=0, dtype=np.float64)\n",
    "                    * np.sum((evaluation - obs_mean) ** 2,\n",
    "                             dtype=np.float64))\n",
    "    r = r_num / r_den\n",
    "    # calculate error in spread of flow alpha\n",
    "    alpha = np.std(simulations, axis=0) / np.std(evaluation, dtype=np.float64)\n",
    "    # calculate error in volume beta (bias of mean discharge)\n",
    "    beta = (np.sum(simulations, axis=0, dtype=np.float64)\n",
    "            / np.sum(evaluation, dtype=np.float64))\n",
    "    # calculate the Kling-Gupta Efficiency KGE\n",
    "    kge_ = 1 - np.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)\n",
    "\n",
    "    return np.vstack((kge_, r, alpha, beta))\n",
    "\n",
    "# Takes pandas df and lists with strings which denote target and feature columns, as well as a desired sequence length.\n",
    "# inherits from pytorch Datafram function\n",
    "# outputs a list containing 2 tensors: 1 with features (including the respective sequence) and 1 with the corresponding target(label)\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, target, features, sequence_length=5):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.y = torch.tensor(dataframe[target].values).to(cuda0).float()\n",
    "        self.X = torch.tensor(dataframe[features].values).to(cuda0).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i): \n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = self.X[i_start:(i + 1), :]\n",
    "        else:\n",
    "            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "            x = self.X[0:(i + 1), :]\n",
    "            x = torch.cat((padding, x), 0)\n",
    "\n",
    "        return x, self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf9597d-8993-476f-8a83-0c5ad3187f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22.04.22 \n",
    "# LSTM model taken from Ralfs version 1.1\n",
    "# Reviewed by Jean-Paul 26.04.22\n",
    "\n",
    "\n",
    "class ShallowRegressionLSTM(nn.Module):                                                                       # define new class which inherits from the nn.Module                                           \n",
    "    def __init__(self, input_size, hidden_size, num_layer, drop_out):                                                   # define constructor, includes objects attributs which must be defined when instantiating given object\n",
    "        super().__init__()                                                                                    \n",
    "        self.input_size = input_size                                                                          # this is the number of features\n",
    "        self.hidden_size = hidden_size                                                                        # number of nodes per layer (identical layer size over ll layers, otherwise multiple attributes for hidden size )\n",
    "        self.num_layers = num_layer                                                                           # number of layers per instantiated nn.LSTM (1 here)\n",
    "        #self.drop_out = drop_out\n",
    "        \n",
    "        self.lstm_1 = nn.LSTM(input_size = input_size,\n",
    "                              hidden_size = hidden_size,\n",
    "                              batch_first = True,\n",
    "                              num_layers=num_layer,\n",
    "                              #drop_out = drop_out\n",
    "                             )\n",
    "        #self.lstm_2 = nn.LSTM(input_size = hidden_size,                                                       # input for the second layer is the output of the previous layer i.e. the hidden states of the previous layer (size=hiden_size)\n",
    "         #                     hidden_size = hidden_size,\n",
    "          #                    batch_first = True,\n",
    "           #                   num_layers=num_layer)\n",
    "            \n",
    "        self.dropout = torch.nn.Dropout(drop_out)                                                             # dropout layer for regualitation http://jmlr.org/papers/v15/srivastava14a.html\n",
    "        \n",
    "        \n",
    "        self.linear = nn.Linear(in_features=self.hidden_size,                                                 # linear or so called Dense-layer, which takes hidden states from previous layer (size=lstm_2(hidden_size)) as input and calculates an output (size=output_features) with linear transformation (y=W*x+b)\n",
    "                                out_features=1)                                                               # and calculates an output (size=output_features) with linear transformation (y=W*x+b)           \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).requires_grad_().to(cuda0)            # initial hidden_states with zeros, requires_grad let pytorch track operations on tensor, to(cuda0) moves tensor to your device (here: GPU)\n",
    "        #h1 = torch.zeros(self.num_layers, batch_size, self.hidden_size).requires_grad_().to(cuda0)            # initial cell states\n",
    "        \n",
    "        # initialize the cell state:\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).requires_grad_().to(cuda0)\n",
    "        #c1 = torch.zeros(self.num_layers, batch_size, self.hidden_size).requires_grad_().to(cuda0)\n",
    "        \n",
    "        out, (hn_1, cn_1) = self.lstm_1(x, (h0.detach(), c0.detach()))                                        # out: batch_size, sequence_length, hidden_size; h_n & c_n: num_layers, hidden_size\n",
    "        #out = self.dropout(out)                                                                               # detach????\n",
    "        #out, (hn_2, cn_2) = self.lstm_2(out, (h1.detach(), c1.detach()))                                      \n",
    "        out = out[:,-1,:]                                                                                     # many to one / sequence to one\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear(out)  # First dim of Hn is num_layers, which is set to 1 above.\n",
    "\n",
    "        return out, (hn_1.transpose(0,1), cn_1.transpose(0,1)),\n",
    "                    #(hn_2.transpose(0,1), cn_2.transpose(0,1)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "414ed993-3f1c-41d5-bd50-271d90502742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found: ['HuewelerbachinclERA5.txt', 'SchwebichinclERA5.txt', '.ipynb_checkpoints', 'WeierbachinclERA5.txt', 'MierbechinclERA5.txt', 'BibeschbachinclERA5.txt']\n",
      "Bibeschbach dataset column names ['P_Roeser', 'T_Roeser', 'Q_Bibeschbach', 'EVAP_Roeser']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a drectory for  your datafiles which containes filenames \n",
    "data_directory = os.listdir(path='/pfs/data5/home/kit/stud/updzl/data')\n",
    "print('Files found: '+str(data_directory))\n",
    "data_directory.pop(2)\n",
    "# Define the current folder with the files \n",
    "os.chdir('/pfs/data5/home/kit/stud/updzl/data')\n",
    "# loop through the directory and load the dataframes directly into local memory. filenames become df names, the .replace method can\n",
    "# change those names. check the read_csv method for argumentexplanations, they differ on the strucutre of your data set \n",
    "# (header = line of column names etc.)\n",
    "for i in range(len(data_directory)):\n",
    "   locals()[str(data_directory[i]).replace('inclERA5.txt', '')]= pd.read_csv(str(data_directory[i]), header=0, parse_dates=[0], index_col =[0])\n",
    "\n",
    "# Define Hyperparameters in dictionary \n",
    "# check http://arxiv.org/abs/1206.5533 for a comrehensive overview over the different hyperparameters involved and their impact\n",
    "\n",
    "model_params = {\n",
    "  \"batch_size\": 64,\n",
    "  \"batch_size_e\": 512,\n",
    "  \"sequence_length\": 512,\n",
    "  \"hidden_size\": 256,            # set hidden_size of model i.e. number of nodes per hidden layer\n",
    "  \"num_layer\":1,  \n",
    "  \"num_epochs\": 5,                # number of training epochs, start high and use the validation dataset after each epoch to check the performance metric \n",
    "                                # on an out of batch example, choose the number of epoch with the best validation performance and not the best training\n",
    "                                # performance\n",
    "  \"drop_out\": 0.6,              # set drop_out of model for regularization\n",
    "  \"learning_rate\": 0.0005,       # set learning rate which is responsible for how strongly the optimization algorithm adapts the model parameters after\n",
    "                                # a training epoch\n",
    "  \"loss\": torch.nn.MSELoss(),   # set loss function: regression task --> mean squared error, already defined in torch framework, possible to write own\n",
    "  \"set_forget_gate\": 3,  \n",
    "  \"adapt_learning_rate_epoch\": 1,\n",
    "  \"adapt_gamma_learning_rate\": 0.2,\n",
    "  \"adapt_end\": 0 ,\n",
    "  \"swa_start\": 30,            # start stochastic weight averaging\n",
    "  \"swa_learning_rate\": 0.0001,\n",
    "  \"grad_clip\" : 'on',            # add gradient clipping\n",
    "  \"max_norm\" : 1,                #define max gradient\n",
    "}\n",
    "\n",
    "model_params = collections.OrderedDict(model_params)\n",
    "\n",
    "# Define training and testing periods\n",
    "start_train = '2010-10-01 00:00:00'\n",
    "#start_train_dt = datetime.datetime.strptime(start_train, '%Y-%m-%d %H:%M:%S')\n",
    "end_train = '2021-09-30 23:30:00'\n",
    "start_val = '2009-10-01 00:00:00'\n",
    "#start_val_dt = datetime.datetime.strptime(start_val, '%Y-%m-%d %H:%M:%S')\n",
    "end_val = '2010-09-30 23:30:00'\n",
    "start_test = '2004-10-01 00:00:00'\n",
    "#start_test_dt = datetime.datetime.strptime(start_test, '%Y-%m-%d %H:%M:%S')\n",
    "end_test = '2010-09-30 00:00:00'\n",
    "#end_test_dt = datetime.datetime.strptime(end_test, '%Y-%m-%d %H:%M:%S')\n",
    "# Define catchment i.e. the dataframe of interest\n",
    "catchment = Bibeschbach\n",
    "catchment_str = 'Bibeschbach'\n",
    "experiment = 'B'\n",
    "\n",
    "# Drop excessive features(columns in your dataframe you do not want to use)\n",
    "\n",
    "excess_name = [col for col in catchment.columns if 'Inter' in col]\n",
    "catchment.drop(labels=excess_name, axis=1, inplace=True)\n",
    "catchment.drop(labels=['Q_flag', 'T_flag'], axis=1, inplace=True)\n",
    "if experiment == 'A':\n",
    "  catchment.drop(labels=[ 'tcwv','cape', 'cin', 'q', 'LLS', 'DLS',  'swvl2', 'swvl3', 'swvl4',], axis=1, inplace=True)\n",
    "elif experiment == 'B':\n",
    "  catchment.drop(labels=['cape', 'cin', 'kx', 'q', 'rh', 'tcwv', 'windspeed', 'LLS', 'DLS', 'swvl1', 'swvl2', 'swvl3', 'swvl4', ], axis = 1, inplace=True)\n",
    "elif experiment == 'C':\n",
    "  print('Die deutsche Gesellschaft für Mykologie wählte das Judasohr zum Pilz des Jahres 2017.')\n",
    "else:\n",
    "  raise ValueError('No experiment defined')\n",
    "\n",
    "\n",
    "# Adding additional data (i.e. rainfall) from neighbouring catchments (stations)\n",
    "#if catchment_str == 'Bibeschbach':\n",
    "#  neighbouring_catchment_A = Mierbech\n",
    "#  #neighbouring_catchment_B = Schwebich\n",
    "#elif catchment_str == 'Huewelerbach':\n",
    "#  neighbouring_catchment_A = Schwebich\n",
    "#  neighbouring_catchment_B = Weierbach\n",
    "#elif catchment_str == 'Mierbech':\n",
    "#  neighbouring_catchment_A = Bibeschbach\n",
    "#  #neighbouring_catchment_B = Schwebich\n",
    "#elif catchment_str == 'Weierbach':\n",
    "#  neighbouring_catchment_A = Huewelerbach\n",
    "#  neighbouring_catchment_B = Schwebich\n",
    "#elif catchment_str == 'Schwebich':\n",
    "#  neighbouring_catchment_A = Huewelerbach\n",
    "#  neighbouring_catchment_B = Weierbach\n",
    "#else:\n",
    "#  raise ValueError(catchment_str+' not found')\n",
    "\n",
    "#col_name_A = [col for col in neighbouring_catchment_A.columns if 'P_' in col]\n",
    "#col_name_B = [col for col in neighbouring_catchment_B.columns if 'P_' in col]\n",
    "#catchment[str(col_name_A[0])] = neighbouring_catchment_A[col_name_A[0]]\n",
    "#catchment[str(col_name_A[1])] = neighbouring_catchment_A[col_name_A[1]]\n",
    "#catchment[str(col_name_B[0])] = neighbouring_catchment_B[col_name_B[0]]\n",
    "#catchment[str(col_name_B[1])] = neighbouring_catchment_B[col_name_B[1]]\n",
    "\n",
    "print(catchment_str+' dataset column names '+str(list(catchment)))\n",
    "\n",
    "# Split and then Standardize the data (z-standartization)\n",
    "train, val, test = split_data(start_train, end_train, start_val, end_val, start_test, end_test, catchment)\n",
    "train_scaled, stds, mean = local_standartization(train)\n",
    "val_scaled = scale(val, stds, mean)\n",
    "test_scaled = scale(test, stds, mean)\n",
    "\n",
    "# Save feature column names as string in list\n",
    "features = list(train_scaled)\n",
    "# removes target column from feature list\n",
    "features.remove('Q_'+catchment_str)\n",
    "# save target column name\n",
    "target = 'Q_'+catchment_str\n",
    "# Create datasets including tensors of features and targets\n",
    "train_set = SequenceDataset(train_scaled, target, features, sequence_length=model_params['sequence_length'])\n",
    "val_set = SequenceDataset(val_scaled, target, features, sequence_length=model_params['sequence_length'])\n",
    "test_set = SequenceDataset(test_scaled, target, features, sequence_length=model_params['sequence_length'])\n",
    "\n",
    "# delete the first int[sequence_length] from datasets\n",
    "\n",
    "train_set.X = train_set.X[model_params['sequence_length']:, :]\n",
    "val_set.X = val_set.X[model_params['sequence_length']:, :]\n",
    "test_set.X = test_set.X[model_params['sequence_length']:, :]\n",
    "train_set.y = train_set.y[model_params['sequence_length']:, ]\n",
    "val_set.y = val_set.y[model_params['sequence_length']:, ]\n",
    "test_set.y = test_set.y[model_params['sequence_length']:, ]\n",
    "\n",
    "#For reproducibility\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "\n",
    "# Insert datasets into pytorchs own DataLoader class, which automates the batching of the sammples for later use in the actual model\n",
    "train_loader = DataLoader(train_set, batch_size = model_params['batch_size'], shuffle = True,  drop_last = True, num_workers=0, worker_init_fn=_init_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=model_params['batch_size'], shuffle = False, drop_last = True, num_workers=0, worker_init_fn=_init_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=model_params['batch_size_e'], shuffle = False, drop_last = True, num_workers=0, worker_init_fn=_init_fn)\n",
    "\n",
    "\n",
    "# Schwebich\n",
    "#load model\n",
    "os.chdir('/pfs/data5/home/kit/stud/updzl/Experiment 1/Best_Performances/'+catchment_str)\n",
    "ann_model = ShallowRegressionLSTM(input_size = len(features),\n",
    "                       hidden_size = model_params[\"hidden_size\"],\n",
    "                       num_layer=1,\n",
    "                       drop_out= model_params[\"drop_out\"]\n",
    "                                 ).to(cuda0)\n",
    "#schwebich_model_A = torch.load(ann_model.state_dict(), '20220509-160643_Schwebich_model_A')\n",
    "ann_model.load_state_dict(torch.load('20220726-142115_Bibeschbach_model_B_s9_Epoch_2', map_location=cuda0,  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8563b874-2875-44fe-8c81-8231ef17dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model.eval()\n",
    "\n",
    "output = np.array([])\n",
    "\n",
    "# save cell state\n",
    "nCols = model_params['hidden_size']\n",
    "df_c0 = pd.DataFrame(index=range(0),columns=range(nCols))\n",
    "#df_c1 = pd.DataFrame(index=range(0),columns=range(nCols))\n",
    "\n",
    "df_h0 = pd.DataFrame(index=range(0),columns=range(nCols))\n",
    "#df_h1 = pd.DataFrame(index=range(0),columns=range(nCols))\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch, _ in test_loader:\n",
    "            y_pred, (h0, c0) = ann_model(batch) # forward call\n",
    "            new = y_pred.flatten().cpu().detach().numpy()\n",
    "            output = np.append(output, new)\n",
    "\n",
    "            # save cell state 0\n",
    "            c0_new = c0.cpu().detach().numpy()\n",
    "            ob_c0 = pd.DataFrame(c0_new.reshape(model_params['batch_size_e'], model_params['hidden_size']))\n",
    "            df_c0 = pd.concat([df_c0, ob_c0], axis=0)\n",
    "\n",
    "            # save cell state 1\n",
    "            #c1_new = c1.cpu().detach().numpy()\n",
    "            #ob_c1 = pd.DataFrame(c1_new.reshape(model_params['batch_size'], model_params['hidden_size']))\n",
    "            #df_c1 = pd.concat([df_c1, ob_c1], axis=0)\n",
    "\n",
    "            # save hidden state 0\n",
    "            h0_new = h0.cpu().detach().numpy()\n",
    "            ob_h0 = pd.DataFrame(h0_new.reshape(model_params['batch_size_e'], model_params['hidden_size']))\n",
    "            df_h0 = pd.concat([df_h0, ob_h0], axis=0)\n",
    "\n",
    "            # save hidden state \n",
    "            #h1_new = h1.cpu().detach().numpy()\n",
    "            #ob_h1 = pd.DataFrame(h1_new.reshape(model_params['batch_size'], model_params['hidden_size']))\n",
    "            #df_h1 = pd.concat([df_h1, ob_h1], axis=0)       \n",
    "        \n",
    "#output_rescaled=(output*stds.loc['Q_'+catchment_str])+mean.loc['Q_'+catchment_str]\n",
    "\n",
    "df_c0 = df_c0.reset_index(drop=True)\n",
    "#df_c1 = df_c1.reset_index(drop=True)\n",
    "df_h0 = df_h0.reset_index(drop=True)\n",
    "#df_h1 = df_h1.reset_index(drop=True)\n",
    "\n",
    "os.chdir('/pfs/data5/home/kit/stud/updzl/Experiment 2/cell_states/')\n",
    "np.savetxt(catchment_str+'_c0_'+experiment+'_best', df_c0, delimiter=',')\n",
    "#np.savetxt(timestr+'_'+catchment_str+'_c1_'+experiment, df_c1, delimiter=',')\n",
    "os.chdir('/pfs/data5/home/kit/stud/updzl/Experiment 2/hidden_states/')\n",
    "np.savetxt(catchment_str+'_h0_'+experiment+'_best', df_h0, delimiter=',')\n",
    "#np.savetxt(timestr+'_'+catchment_str+'_h1_'+experiment, df_h1, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd3251-f7d2-4606-8f4c-8e530f5a712b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
